{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUT LG CLASSIC NO GR~~HUT LARGE PIZZA~~L SF NG...</td>\n",
       "      <td>['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREP BASE SAUCE/CHS~~PREP BUILD~~L SC 170G PLA...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UE. DEALS~~UE. DEALS~~UE C2 DEAL 1 MEDIUM PIZZA</td>\n",
       "      <td>['O', 'C', 'O', 'C', 'O', 'O', 'O', 'O', 'S', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DV. LG CLASSIC NO GR~~DV. LARGE PIZZA~~LARGE S...</td>\n",
       "      <td>['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'S', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UE. PREMIUM SIDES~~UE. SIDES~~HOT HONEY BBQ CH...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>JE. TOPPINGS DEFAULT~~JE. TOPPINGS~~P PLANT (E...</td>\n",
       "      <td>['O', 'C', 'O', 'O', 'C', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>DV. MED PIZZA~~DV. MED PIZZA~~MEDIUM DEAL 1 PL...</td>\n",
       "      <td>['O', 'S', 'C', 'O', 'S', 'C', 'S', 'C', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0007-SOFT BEVERAGE~~PURCHASED ITEMS~~JIMMY'S I...</td>\n",
       "      <td>['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>UE. LG PAN~~UE. LARGE PIZZA~~YUM L PAN MEAT SU...</td>\n",
       "      <td>['O', 'S', 'B', 'O', 'S', 'C', 'O', 'O', 'B', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>JE. LG PIZZA~~JE. LARGE PIZZA~~LARGE PLANT VEG...</td>\n",
       "      <td>['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0   HUT LG CLASSIC NO GR~~HUT LARGE PIZZA~~L SF NG...   \n",
       "1   PREP BASE SAUCE/CHS~~PREP BUILD~~L SC 170G PLA...   \n",
       "2     UE. DEALS~~UE. DEALS~~UE C2 DEAL 1 MEDIUM PIZZA   \n",
       "3   DV. LG CLASSIC NO GR~~DV. LARGE PIZZA~~LARGE S...   \n",
       "4   UE. PREMIUM SIDES~~UE. SIDES~~HOT HONEY BBQ CH...   \n",
       "..                                                ...   \n",
       "95  JE. TOPPINGS DEFAULT~~JE. TOPPINGS~~P PLANT (E...   \n",
       "96  DV. MED PIZZA~~DV. MED PIZZA~~MEDIUM DEAL 1 PL...   \n",
       "97  0007-SOFT BEVERAGE~~PURCHASED ITEMS~~JIMMY'S I...   \n",
       "98  UE. LG PAN~~UE. LARGE PIZZA~~YUM L PAN MEAT SU...   \n",
       "99  JE. LG PIZZA~~JE. LARGE PIZZA~~LARGE PLANT VEG...   \n",
       "\n",
       "                                                Label  \n",
       "0   ['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'O', ...  \n",
       "1   ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "2   ['O', 'C', 'O', 'C', 'O', 'O', 'O', 'O', 'S', ...  \n",
       "3   ['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'S', ...  \n",
       "4   ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \n",
       "..                                                ...  \n",
       "95  ['O', 'C', 'O', 'O', 'C', 'O', 'O', 'O', 'O', ...  \n",
       "96  ['O', 'S', 'C', 'O', 'S', 'C', 'S', 'C', 'O', ...  \n",
       "97           ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
       "98  ['O', 'S', 'B', 'O', 'S', 'C', 'O', 'O', 'B', ...  \n",
       "99  ['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', ...  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "main_df = pd.read_csv(r\"data\\ner_data_20_5_part3.csv\")\n",
    "main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validating tags length and sentence length in dataset given by gpt-4\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "def validating_data(dataframe: pd.DataFrame) -> dict[str, int | list[int]]:\n",
    "    unequal_rows = []\n",
    "    for idx in range(len(dataframe)):\n",
    "        length_words = len(\" \".join(tuple(dataframe.iloc[idx,:].values)[0].split(\"~~\")).strip().split(\" \"))\n",
    "        length_tags = len(literal_eval(tuple(dataframe.iloc[idx,:].values)[1]))\n",
    "\n",
    "        if length_words != length_tags:\n",
    "            unequal_rows.append(idx)\n",
    "\n",
    "    return {\"rows\": unequal_rows, \"length\": len(unequal_rows)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HUT INSTRUCTION~~HUT DEALS~~</td>\n",
       "      <td>['O', 'O', 'O', 'C']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUT SML CLASSIC~~HUT SMALL PIZZA~~</td>\n",
       "      <td>['O', 'S', 'B', 'O', 'S', 'C']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0001-PIZZA FOOD~~PURCHASED ITEMS~~RAPESEED OIL...</td>\n",
       "      <td>['C', 'O', 'O', 'O', 'O', 'O', 'O']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HUT INSTRUCTION~~HUT DEALS~~</td>\n",
       "      <td>['O', 'O', 'O', 'C']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HUT INSTRUCTION~~HUT DEALS~~</td>\n",
       "      <td>['O', 'O', 'O', 'C']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>JE. MED PIZZA~~JE. MED PIZZA~~MEDIUM DEAL 1 PL...</td>\n",
       "      <td>['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>UE. MED CLASSIC NO G~~UE. MED PIZZA~~M CLC NG ...</td>\n",
       "      <td>['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>PHD SML CLASSIC NO G~~PHD SMALL PIZZA~~SMALL S...</td>\n",
       "      <td>['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'S', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>HUT TOPPINGS DEFAULT~~HUT TOPPINGS~~S CLC 1G H...</td>\n",
       "      <td>['O', 'C', 'O', 'O', 'C', 'O', 'O', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>DV. SML PIZZA~~DV. SMALL PIZZA~~SMALL DEAL 2 P...</td>\n",
       "      <td>['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Sentence  \\\n",
       "0                         HUT INSTRUCTION~~HUT DEALS~~   \n",
       "1                   HUT SML CLASSIC~~HUT SMALL PIZZA~~   \n",
       "2    0001-PIZZA FOOD~~PURCHASED ITEMS~~RAPESEED OIL...   \n",
       "3                         HUT INSTRUCTION~~HUT DEALS~~   \n",
       "4                         HUT INSTRUCTION~~HUT DEALS~~   \n",
       "..                                                 ...   \n",
       "275  JE. MED PIZZA~~JE. MED PIZZA~~MEDIUM DEAL 1 PL...   \n",
       "276  UE. MED CLASSIC NO G~~UE. MED PIZZA~~M CLC NG ...   \n",
       "277  PHD SML CLASSIC NO G~~PHD SMALL PIZZA~~SMALL S...   \n",
       "278  HUT TOPPINGS DEFAULT~~HUT TOPPINGS~~S CLC 1G H...   \n",
       "279  DV. SML PIZZA~~DV. SMALL PIZZA~~SMALL DEAL 2 P...   \n",
       "\n",
       "                                                 Label  \n",
       "0                                 ['O', 'O', 'O', 'C']  \n",
       "1                       ['O', 'S', 'B', 'O', 'S', 'C']  \n",
       "2                  ['C', 'O', 'O', 'O', 'O', 'O', 'O']  \n",
       "3                                 ['O', 'O', 'O', 'C']  \n",
       "4                                 ['O', 'O', 'O', 'C']  \n",
       "..                                                 ...  \n",
       "275  ['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', ...  \n",
       "276  ['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'O', ...  \n",
       "277  ['O', 'S', 'B', 'O', 'O', 'O', 'S', 'C', 'S', ...  \n",
       "278  ['O', 'C', 'O', 'O', 'C', 'O', 'O', 'O', 'O', ...  \n",
       "279  ['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', ...  \n",
       "\n",
       "[280 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combining all small datasets together to make one dataset\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = \"data\"\n",
    "dataframe = pd.DataFrame()\n",
    "\n",
    "files = os.listdir(directory)\n",
    "\n",
    "for element in files:\n",
    "    df = pd.read_csv(os.path.join(directory, element))\n",
    "    dataframe = pd.concat([dataframe, df], ignore_index=True)\n",
    "\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting labels to ner_tags (str2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'C']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from ast import literal_eval\n",
    "\n",
    "sentences = [{\"text\": dataframe[\"Sentence\"][i] , \"labels\": literal_eval(dataframe[\"Label\"][i])}  for i in range(len(dataframe))]\n",
    "\n",
    "ds = Dataset.from_pandas(pd.DataFrame(data=sentences))\n",
    "ds[\"labels\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 280/280 [00:00<00:00, 11666.12 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "# Define a Classlabel object to use to map string labels to integers.\n",
    "classmap = ClassLabel(num_classes=4, names=['O', 'C', 'S', 'B'])\n",
    "\n",
    "# Map labels to label ids.\n",
    "ds = ds.map(lambda y: {\"labels\": classmap.str2int(y[\"labels\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 1],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 0, 0, 1],\n",
       " [0, 2, 3, 0, 2, 1],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 2, 3, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0],\n",
       " [0, 2, 3, 3, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 1, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 3, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 1, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 1, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 0, 3, 0, 2, 1, 2, 3, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0, 0, 2, 1],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 3, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 3, 0, 2, 1, 0, 0, 3, 3, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0],\n",
       " [0, 2, 3, 0, 0, 2, 1, 0, 3, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 3, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 3, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 2, 3, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 3, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 3, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 2, 3, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 0, 2, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 1, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 0, 3, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 3, 1, 0, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 2, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 2, 3, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 2, 1, 0, 0, 3, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 2, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 0, 2, 1, 0, 0, 0],\n",
       " [0, 3, 1, 0, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 3, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 3, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 2, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0],\n",
       " [0, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 3, 0, 0, 0, 0, 1],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 3, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 3, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 2, 1, 0, 3, 0, 0, 0],\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 1, 0, 0, 0, 3, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 1, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 3, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 3, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 2, 3, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 2, 1, 0, 0, 3, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
       " [0, 2, 3, 0, 0, 0, 2, 1, 2, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 2, 1, 0, 2, 1, 2, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[108, 110, 149]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = validating_data(dataframe)\n",
    "result[\"rows\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = dataframe.drop(result[\"rows\"], axis=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.drop(\"index\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset\n",
    "# from ast import literal_eval\n",
    "\n",
    "# sentences = [{\"text\": df[\"Sentence\"][i] , \"labels\": literal_eval(df[\"Label\"][i])}  for i in range(len(df))]\n",
    "\n",
    "# ds = Dataset.from_pandas(pd.DataFrame(data=sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 150/150 [00:00<00:00, 7500.99 examples/s]\n",
      "Map: 100%|██████████| 150/150 [00:00<00:00, 11539.30 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# from datasets import ClassLabel\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # Define a tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# # Define a Classlabel object to use to map string labels to integers.\n",
    "# classmap = ClassLabel(num_classes=4, names=['O', 'C', 'S', 'B'])\n",
    "\n",
    "# # Map text to tokenizer ids.\n",
    "# ds = ds.map(lambda x: tokenizer(x[\"text\"], truncation=True))\n",
    "\n",
    "# # Map labels to label ids.\n",
    "# ds = ds.map(lambda y: {\"labels\": classmap.str2int(y[\"labels\"])})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "training_dataset_size = math.ceil(len(clean_df) * 0.8)\n",
    "print(training_dataset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "Map: 100%|██████████| 222/222 [00:00<00:00, 8222.53 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 6738.44 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 222/222 [00:00<00:00, 16290.84 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 54/54 [00:00<00:00, 10645.94 examples/s]\n",
      "d:\\ner model advanced\\env\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "100%|██████████| 7/7 [1:03:38<00:00, 545.49s/it]\n",
      "100%|██████████| 7/7 [04:57<00:00, 42.43s/it]\n",
      "100%|██████████| 84/84 [03:18<00:00,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 198.4413, 'train_samples_per_second': 3.356, 'train_steps_per_second': 0.423, 'train_loss': 0.4551026480538504, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=84, training_loss=0.4551026480538504, metrics={'train_runtime': 198.4413, 'train_samples_per_second': 3.356, 'train_steps_per_second': 0.423, 'train_loss': 0.4551026480538504, 'epoch': 3.0})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from ast import literal_eval\n",
    "import math\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import ClassLabel\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, Trainer, AutoTokenizer, DataCollatorForTokenClassification\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Define a Classlabel object to use to map string labels to integers.\n",
    "classmap = ClassLabel(num_classes=4, names=['O', 'C', 'S', 'B'])\n",
    "\n",
    "total_sentences = [{\"text\": clean_df[\"Sentence\"][i] , \"labels\": literal_eval(clean_df[\"Label\"][i])}  for i in range(len(clean_df))]\n",
    "\n",
    "training_dataset_size = math.ceil(len(clean_df) * 0.8)\n",
    "\n",
    "train_sentences = total_sentences[:training_dataset_size]\n",
    "\n",
    "eval_sentences = total_sentences[training_dataset_size + 1:]\n",
    "\n",
    "ds_train = Dataset.from_pandas(pd.DataFrame(data=train_sentences))\n",
    "ds_eval = Dataset.from_pandas(pd.DataFrame(data=eval_sentences))\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "                                                        id2label={i:classmap.int2str(i) for i in range(classmap.num_classes)},\n",
    "                                                        label2id={c:classmap.str2int(c) for c in classmap.names},\n",
    "                                                        finetuning_task=\"ner\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "\n",
    "ds_train = ds_train.map(lambda x: tokenizer(x[\"text\"], truncation=True))\n",
    "ds_eval = ds_eval.map(lambda x: tokenizer(x[\"text\"], truncation=True))\n",
    "\n",
    "ds_train = ds_train.map(lambda y: {\"labels\": classmap.str2int(y[\"labels\"])})\n",
    "ds_eval = ds_eval.map(lambda y: {\"labels\": classmap.str2int(y[\"labels\"])})\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(axis=2)\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_eval,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 222\n",
       "})"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"ner_data_complete.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " '##e',\n",
       " '.',\n",
       " 'l',\n",
       " '##g',\n",
       " 'pizza',\n",
       " '~',\n",
       " '~',\n",
       " 'u',\n",
       " '##e',\n",
       " '.',\n",
       " 'large',\n",
       " 'pizza',\n",
       " '~',\n",
       " '~',\n",
       " 'l',\n",
       " 'sf',\n",
       " 'meat',\n",
       " 'supreme',\n",
       " 'crust']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('UE. LG PIZZA~~UE. LARGE PIZZA~~L SF MEAT SUPREME CRUST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HUT LG CLASSIC NO GR~~HUT LARGE PIZZA~~L SF NG HOT HONEY PEPPERONI FEAST'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_df[\"Sentence\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'hut',\n",
       " 'l',\n",
       " '##g',\n",
       " 'classic',\n",
       " 'no',\n",
       " 'gr',\n",
       " '~',\n",
       " '~',\n",
       " 'hut',\n",
       " 'large',\n",
       " 'pizza',\n",
       " '~',\n",
       " '~',\n",
       " 'l',\n",
       " 'sf',\n",
       " 'ng',\n",
       " 'hot',\n",
       " 'honey',\n",
       " 'pepper',\n",
       " '##oni',\n",
       " 'feast',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(main_df[\"Sentence\"][0])\n",
    "inputs.tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 17,\n",
       " 18,\n",
       " None]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        # print(word_id)\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            # print(\"loop word id\", word_id)\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "            # print(new_labels)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            # if label % 2 == 1:\n",
    "            #     print(\"else\")\n",
    "            #     label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '000',\n",
       " '##2',\n",
       " '-',\n",
       " 'pizza',\n",
       " 'topping',\n",
       " '##s',\n",
       " 'purchased',\n",
       " 'items',\n",
       " 'z',\n",
       " '##tur',\n",
       " '##key',\n",
       " 'stamp',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\" \".join(ds[\"text\"][101].split(\"~~\")).strip()).tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, None]\n",
      "['[CLS]', 'hut', 'instruction', 'hut', 'deals', '[SEP]']\n",
      "[0, 0, 0, 1]\n",
      "[-100, 0, 0, 0, 1, -100]\n"
     ]
    }
   ],
   "source": [
    "labels = ds[\"labels\"][0]\n",
    "sentence = \" \".join(ds[\"text\"][0].split(\"~~\")).strip()\n",
    "inputs = tokenizer(sentence)\n",
    "word_ids = inputs.word_ids()\n",
    "tokens = inputs.tokens()\n",
    "print(word_ids)\n",
    "print(tokens)\n",
    "print(labels)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(ds_eval)[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "NameError",
     "evalue": "name 'label_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_eval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\ner model advanced\\env\\Lib\\site-packages\\transformers\\trainer.py:3305\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[1;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3302\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3304\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3305\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPrediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\n\u001b[0;32m   3307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3308\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32md:\\ner model advanced\\env\\Lib\\site-packages\\transformers\\trainer.py:3520\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3517\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3518\u001b[0m         )\n\u001b[0;32m   3519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3520\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3522\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[50], line 47\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     45\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Remove ignored index (special tokens)\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     51\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     52\u001b[0m     [label_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     54\u001b[0m ]\n\u001b[0;32m     55\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n",
      "Cell \u001b[1;32mIn[50], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Remove ignored index (special tokens)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 48\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     50\u001b[0m ]\n\u001b[0;32m     51\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     52\u001b[0m     [label_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     54\u001b[0m ]\n\u001b[0;32m     55\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n",
      "Cell \u001b[1;32mIn[50], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m predictions \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Remove ignored index (special tokens)\u001b[39;00m\n\u001b[0;32m     47\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 48\u001b[0m     [\u001b[43mlabel_list\u001b[49m[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     50\u001b[0m ]\n\u001b[0;32m     51\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     52\u001b[0m     [label_list[l] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     54\u001b[0m ]\n\u001b[0;32m     55\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label_list' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(ds_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "new_model = AutoModelForTokenClassification.from_pretrained(\"finetuned_ner\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: UE. LG PIZZA~~UE. LARGE PIZZA~~LARGE MEAT SUPREME\n",
      "Actual Labels: ['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O']\n",
      "UE. LG PIZZA UE. LARGE PIZZA LARGE MEAT SUPREME\n",
      "Predicted Labels: ['O', 'S', 'C', 'O', 'S', 'C', 'S', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B']\n"
     ]
    }
   ],
   "source": [
    "n = 90\n",
    "\n",
    "input_text = clean_df[\"Sentence\"][n]\n",
    "\n",
    "print(\"Text:\", input_text)\n",
    "\n",
    "print(\"Actual Labels:\", clean_df[\"Label\"][n])\n",
    "input_text_preprocessed = \" \".join(input_text.split(\"~~\")).strip()\n",
    "print(input_text_preprocessed)\n",
    "\n",
    "# Prepare input\n",
    "inputs = tokenizer(input_text_preprocessed, return_tensors=\"pt\", truncation=True)\n",
    "\n",
    "# Perform inference\n",
    "outputs = new_model(**inputs)\n",
    "\n",
    "# Post-process output\n",
    "predicted_labels = outputs.logits.argmax(dim=2)\n",
    "\n",
    "def indices_to_labels(indices, classmap):\n",
    "    labels = [classmap.int2str(idx) for idx in indices]\n",
    "    return labels\n",
    "\n",
    "predicted_labels_classes = indices_to_labels(predicted_labels.squeeze().tolist(), classmap)\n",
    "\n",
    "print(\"Predicted Labels:\", predicted_labels_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['u', '##e', '.', 'l', '##g', 'pizza', '~', '~', 'u', '##e', '.', 'large', 'pizza', '~', '~', 'large', 'meat', 'supreme']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"UE. LG PIZZA~~UE. LARGE PIZZA~~LARGE MEAT SUPREME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_labels_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
